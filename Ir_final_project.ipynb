{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to\n",
      "[nltk_data]     /Users/sarthak.jain/nltk_data...\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "nltk.download('reuters')\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import string\n",
    "import random\n",
    "import numpy as np\n",
    "from math import pow\n",
    "from math import sqrt\n",
    "from math import ceil\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define our tokenizer\n",
    "table = str.maketrans('', '', string.punctuation) #removes punctuation when used with translate\n",
    "def tokenize(text):\n",
    "    tokens = nltk.word_tokenize(text.lower().translate(table)) #lowe-case text, remove punctuation, tokenize\n",
    "    stemmed_tokens = [PorterStemmer().stem(item) for item in tokens] #Porter Stemming\n",
    "    return stemmed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Preprocess documents\n",
    "file_ids = reuters.fileids() #reuter file ids\n",
    "original_doc_list = [reuters.raw(file_id) for file_id in file_ids] #original document content\n",
    "mod_doc_list = [doc[(doc.index('\\n') + 1):] for doc in original_doc_list] #document with title removed\n",
    "no_of_docs = len(mod_doc_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "//anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['abov', 'afterward', 'alon', 'alreadi', 'alway', 'ani', 'anoth', 'anyon', 'anyth', 'anywher', 'becam', 'becaus', 'becom', 'befor', 'besid', 'cri', 'describ', 'dure', 'els', 'elsewher', 'empti', 'everi', 'everyon', 'everyth', 'everywher', 'fifti', 'formerli', 'forti', 'ha', 'henc', 'hereaft', 'herebi', 'hi', 'howev', 'hundr', 'inde', 'latterli', 'mani', 'meanwhil', 'moreov', 'mostli', 'nobodi', 'noon', 'noth', 'nowher', 'onc', 'onli', 'otherwis', 'ourselv', 'perhap', 'pleas', 'seriou', 'sever', 'sinc', 'sincer', 'sixti', 'someon', 'someth', 'sometim', 'somewher', 'themselv', 'thenc', 'thereaft', 'therebi', 'therefor', 'thi', 'thu', 'togeth', 'twelv', 'twenti', 'veri', 'wa', 'whatev', 'whenc', 'whenev', 'wherea', 'whereaft', 'wherebi', 'wherev', 'whi', 'yourselv'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
       "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
       "                min_df=1, ngram_range=(1, 1), norm='l2', preprocessor=None,\n",
       "                smooth_idf=True, stop_words='english', strip_accents=None,\n",
       "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "                tokenizer=<function tokenize at 0x1a15ec2620>, use_idf=True,\n",
       "                vocabulary=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate idf value\n",
    "vectorizer = TfidfVectorizer(tokenizer=tokenize, stop_words='english', use_idf=True)\n",
    "corpus = mod_doc_list\n",
    "vectorizer.fit(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create idf dictionary\n",
    "features = vectorizer.get_feature_names()\n",
    "idfarray = vectorizer.idf_\n",
    "idf_dict = {}\n",
    "for i in range(0, len(features)):\n",
    "    idf_dict[features[i]] = idfarray[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Return similrity matrix using idf-modified-cosine as similaity measure\n",
    "def SimilarityMatrix(sent_text):\n",
    "    no_of_sentences = len(sent_text)\n",
    "    similarity_mat = np.zeros((no_of_sentences,no_of_sentences))  #initialized with zeroes\n",
    "    for i in range(0, no_of_sentences):\n",
    "        for j in range(0, no_of_sentences):\n",
    "            if i == j:\n",
    "                similarity_mat[i][j] = 1\n",
    "            else:\n",
    "                lis1 = tokenize(sent_text[i]) #token list for sentence i\n",
    "                lis2 = tokenize(sent_text[j]) #token list for sentence j\n",
    "                dict1 = Counter(lis1) #dictionary of term frequencies(term->tf)\n",
    "                dict2 = Counter(lis2) #dictionary of term frequencies(term->tf)\n",
    "                common_tokens = list(set(lis1 + lis2)) #common tokens in the 2 sentences\n",
    "\n",
    "                numerator = 0\n",
    "                for term in common_tokens:\n",
    "                    if term in idf_dict:\n",
    "                        numerator += dict1[term] * dict2[term] * idf_dict[term] * idf_dict[term]\n",
    "\n",
    "                denom1 = 0\n",
    "                denom2 = 0\n",
    "                for token in lis1:\n",
    "                    if token in idf_dict:\n",
    "                        denom1 += pow(dict1[token] * idf_dict[token], 2)\n",
    "                for token in lis2:\n",
    "                    if token in idf_dict:\n",
    "                        denom2 += pow(dict2[token] * idf_dict[token], 2)\n",
    "                denominator = sqrt(denom1 * denom2)\n",
    "\n",
    "                similarity_mat[i][j] = numerator / denominator #idf-modified-cosine\n",
    "\n",
    "    return similarity_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply page rank algorithm and return pagerank array based on the similarity matrix\n",
    "def PageRank(similarity_mat):\n",
    "    no_of_sentences = np.shape(similarity_mat)[0]\n",
    "    #Calculate rowsum array i.e. the sum of similarity values for each sentence excluding the sentence itself\n",
    "    rowsum_array = [sum(similarity_mat[i]) - similarity_mat[i][i] for i in range(0, no_of_sentences)]\n",
    "\n",
    "    #Initialize pagerank array with value (1/no_of_sentences) for each sentence\n",
    "    pagerank_array = [(1 / no_of_sentences)] * no_of_sentences\n",
    "\n",
    "    iterations_threshold = 100000 #No of consecutive iterations for stopping\n",
    "    pages_threshold = no_of_sentences #Threshold on the no of pages to be counted as a stable iteration\n",
    "    percentage_threshold = pow(10, -250) #Threshold on the percentage to be counted as a stable page\n",
    "    no_of_iterations = 200000 #No of total iterations after which to terminate the loop\n",
    "\n",
    "    consecutive_iterations = 0 #No of consecutive stable iterations at any point\n",
    "    iterations_count = 0 #Total iterations till now\n",
    "\n",
    "    while consecutive_iterations < iterations_threshold and iterations_count < no_of_iterations:\n",
    "        #Caclulate new pageranks at (i+1)th iteration\n",
    "        mod_pagerank = []\n",
    "        for i in range(0, no_of_sentences):\n",
    "            new_pagerank_value = 0.85 / no_of_sentences\n",
    "            idf_similarity_sum = 0\n",
    "            for j in range(0, no_of_sentences):\n",
    "                if i != j and rowsum_array[j] != 0:\n",
    "                    idf_similarity_sum += (similarity_mat[i][j] * pagerank_array[j]) / rowsum_array[j]\n",
    "            new_pagerank_value += 0.15 * idf_similarity_sum\n",
    "            mod_pagerank.append(new_pagerank_value)\n",
    "\n",
    "        no_of_positives = 0 #No of pages having change less than the threshold in the current iteration\n",
    "        for i in range(0, len(pagerank_array)):\n",
    "            abs_difference = abs(mod_pagerank[i] - pagerank_array[i])\n",
    "            if abs_difference <= (percentage_threshold * pagerank_array[i]):\n",
    "                no_of_positives += 1\n",
    "        if no_of_positives >= ceil(pages_threshold):\n",
    "            consecutive_iterations += 1\n",
    "        else:\n",
    "            consecutive_iterations = 0\n",
    "\n",
    "        iterations_count += 1\n",
    "        pagerank_array = mod_pagerank\n",
    "        \n",
    "    return pagerank_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The document is:\n",
      "  Algeria will tender on April 3 for\n",
      "  20,000 tonnes of optional origin sunflowerseed oil/rapeseed oil\n",
      "  for Apr/May loading, traders said.\n",
      "      Meanwhile, the market is awaiting results of an Algerian\n",
      "  import tender which took place over the weekend for about\n",
      "  10,000 tonnes of refined vegetable oils in drums, traders\n",
      "  added.\n",
      "  \n",
      "\n",
      "\n",
      "The original title is:\n",
      "ALGERIA SETS TENDER FOR RAPE/SUNFLOWERSEED OIL\n",
      "\n",
      "The title according to lexrank is:\n",
      "  Algeria will tender on April 3 for\n",
      "  20,000 tonnes of optional origin sunflowerseed oil/rapeseed oil\n",
      "  for Apr/May loading, traders said.\n"
     ]
    }
   ],
   "source": [
    "#Select a random document \n",
    "doc_no = random.randint(0, no_of_docs)\n",
    "#doc_no=8425\n",
    "text = mod_doc_list[doc_no]\n",
    "sent_text = nltk.sent_tokenize(text)\n",
    "similarity_matrix = SimilarityMatrix(sent_text)\n",
    "pagerank_array = PageRank(similarity_matrix)\n",
    "print(\"The document is:\")\n",
    "print(text)\n",
    "\n",
    "\n",
    "original_title=''\n",
    "complete_text=reuters.raw(file_ids[doc_no])\n",
    "\n",
    "j=0\n",
    "while(complete_text[j] != '\\n'):\n",
    "        original_title=original_title+complete_text[j]\n",
    "        j = j + 1\n",
    "print(\"The original title is:\")\n",
    "print(original_title)\n",
    "print()\n",
    "\n",
    "\n",
    "print(\"The title according to lexrank is:\")\n",
    "print(sent_text[pagerank_array.index(max(pagerank_array))])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
